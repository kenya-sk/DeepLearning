{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN(Convolutional Neural Network)の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.append(\"../../deep-learning-from-scratch/\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2col (image to column)関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    input_data: 4D array (data size, channel, height, width)\n",
    "    filter_h: height of filter\n",
    "    filter_w: width of filter\n",
    "    stride: stride\n",
    "    pad: pading\n",
    "    \"\"\"\n",
    "    \n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2*pad - filter_w) // stride + 1\n",
    "    \n",
    "    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], \"constant\")\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "    \n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "            \n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1.shape: (9, 75)\n",
      "col2.shape: (90, 75)\n"
     ]
    }
   ],
   "source": [
    "#im2col関数の練習\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(\"col1.shape: {}\".format(col1.shape))\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(\"col2.shape: {}\".format(col2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#逆の処理　col2im(column to image)関数の実装\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2*pad - filter_w) // stride +1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "    \n",
    "    return img[:, :, pad:H +pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolution Layer の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        #中間データ\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        #重み・バイアスパラメータの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "       \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW =  self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        \n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Layer の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        #画像をチャンネル毎に独立に展開\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        #max pooling のため、最大値を計算\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convolution -> ReLU -> Pooling -> Affine -> ReLU -> Affine -> Softmax\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                            conv_param={\"filter_num\":30, \"filter_size\":5,\n",
    "                            \"pad\":0, \"stride\":1}, \n",
    "                            hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param[\"filter_num\"]\n",
    "        filter_size = conv_param[\"filter_size\"]\n",
    "        filter_pad = conv_param[\"pad\"]\n",
    "        filter_stride = conv_param[\"stride\"]\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2)*(conv_output_size/2))\n",
    "        \n",
    "        #重みパラメータの初期化\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params[\"b1\"] = np.zeros(filter_num)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params[\"b2\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W3\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params[\"b3\"] = np.zeros(output_size)\n",
    "        \n",
    "        #レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Conv1\"] = Convolution(self.params[\"W1\"], self.params[\"b1\"], \n",
    "                                                                       conv_param[\"stride\"], conv_param[\"pad\"])\n",
    "        self.layers[\"Relu1\"] = Relu()\n",
    "        self.layers[\"Pool1\"] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\n",
    "        self.layers[\"Relu2\"] = Relu()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"], self.params[\"b3\"])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "       \n",
    "        acc = 0.0\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "\n",
    "        return acc / x.shape[0]\n",
    "            \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        #forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads[\"W1\"] = self.layers[\"Conv1\"].dW\n",
    "        grads[\"b1\"] = self.layers[\"Conv1\"].db\n",
    "        grads[\"W2\"] = self.layers[\"Affine1\"].dW\n",
    "        grads[\"b2\"] = self.layers[\"Affine1\"].db\n",
    "        grads[\"W3\"] = self.layers[\"Affine2\"].dW\n",
    "        grads[\"b3\"] = self.layers[\"Affine2\"].db\n",
    "        return grads\n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNを用いたMNISTの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.30028045674\n",
      "=== epoch:1, train acc:0.16, test acc:0.173 ===\n",
      "train loss:2.29705122273\n",
      "train loss:2.29336896143\n",
      "train loss:2.28473143353\n",
      "train loss:2.28092040013\n",
      "train loss:2.26842481618\n",
      "train loss:2.24949890123\n",
      "train loss:2.2363635813\n",
      "train loss:2.22829537461\n",
      "train loss:2.20345579775\n",
      "train loss:2.11661393477\n",
      "train loss:2.11942432534\n",
      "train loss:2.03429806192\n",
      "train loss:2.07316116345\n",
      "train loss:1.96659096177\n",
      "train loss:1.89370463227\n",
      "train loss:1.81078105225\n",
      "train loss:1.78826355485\n",
      "train loss:1.66634131167\n",
      "train loss:1.60892807161\n",
      "train loss:1.53786895796\n",
      "train loss:1.46547954273\n",
      "train loss:1.28994482979\n",
      "train loss:1.27197828009\n",
      "train loss:1.23638866196\n",
      "train loss:1.12740046369\n",
      "train loss:1.14216730832\n",
      "train loss:0.909205364243\n",
      "train loss:1.00105800805\n",
      "train loss:0.875470751154\n",
      "train loss:0.820353403573\n",
      "train loss:0.703620677201\n",
      "train loss:0.841433880987\n",
      "train loss:0.832192667165\n",
      "train loss:0.754738109108\n",
      "train loss:0.767927177242\n",
      "train loss:0.878326812386\n",
      "train loss:0.886858218774\n",
      "train loss:0.779518290536\n",
      "train loss:0.685795194546\n",
      "train loss:0.617377121865\n",
      "train loss:0.705107454942\n",
      "train loss:0.544054349629\n",
      "train loss:0.576941371266\n",
      "train loss:0.625864475449\n",
      "train loss:0.596085853709\n",
      "train loss:0.595412319429\n",
      "train loss:0.512573898452\n",
      "train loss:0.648489050485\n",
      "train loss:0.620827661609\n",
      "train loss:0.62908038414\n",
      "=== epoch:2, train acc:0.822, test acc:0.771 ===\n",
      "train loss:0.658350544961\n",
      "train loss:0.542962122521\n",
      "train loss:0.438687524485\n",
      "train loss:0.645803107284\n",
      "train loss:0.649735616637\n",
      "train loss:0.566177429093\n",
      "train loss:0.563385099713\n",
      "train loss:0.509398207123\n",
      "train loss:0.412616510824\n",
      "train loss:0.329326539577\n",
      "train loss:0.369725337712\n",
      "train loss:0.478165089808\n",
      "train loss:0.541690998351\n",
      "train loss:0.486182700611\n",
      "train loss:0.362632968024\n",
      "train loss:0.425931974747\n",
      "train loss:0.38254579781\n",
      "train loss:0.441027621063\n",
      "train loss:0.432761411527\n",
      "train loss:0.64020618799\n",
      "train loss:0.389473283805\n",
      "train loss:0.369856623641\n",
      "train loss:0.49115364321\n",
      "train loss:0.466743676803\n",
      "train loss:0.382580597257\n",
      "train loss:0.349014451466\n",
      "train loss:0.491998091683\n",
      "train loss:0.394259337798\n",
      "train loss:0.405241730781\n",
      "train loss:0.390106981396\n",
      "train loss:0.304948813874\n",
      "train loss:0.491381297746\n",
      "train loss:0.396018554406\n",
      "train loss:0.378433635311\n",
      "train loss:0.312332401612\n",
      "train loss:0.507785631306\n",
      "train loss:0.326002328127\n",
      "train loss:0.269166057999\n",
      "train loss:0.294855357306\n",
      "train loss:0.503347912623\n",
      "train loss:0.393940358983\n",
      "train loss:0.443062949048\n",
      "train loss:0.355178484998\n",
      "train loss:0.42105393278\n",
      "train loss:0.279641655849\n",
      "train loss:0.393858977169\n",
      "train loss:0.321309544674\n",
      "train loss:0.210836299284\n",
      "train loss:0.282882926443\n",
      "train loss:0.341769734222\n",
      "=== epoch:3, train acc:0.867, test acc:0.868 ===\n",
      "train loss:0.284896200271\n",
      "train loss:0.420785700579\n",
      "train loss:0.251252013382\n",
      "train loss:0.325667930661\n",
      "train loss:0.652849088611\n",
      "train loss:0.457707050789\n",
      "train loss:0.430483069404\n",
      "train loss:0.292114852323\n",
      "train loss:0.4219791103\n",
      "train loss:0.227114069975\n",
      "train loss:0.278555509892\n",
      "train loss:0.291499575668\n",
      "train loss:0.423969215258\n",
      "train loss:0.258038045592\n",
      "train loss:0.239368666311\n",
      "train loss:0.501323904955\n",
      "train loss:0.211360702015\n",
      "train loss:0.479878427638\n",
      "train loss:0.377367007246\n",
      "train loss:0.30070065475\n",
      "train loss:0.147084198547\n",
      "train loss:0.331695650648\n",
      "train loss:0.367929255274\n",
      "train loss:0.229327447124\n",
      "train loss:0.272199382483\n",
      "train loss:0.333276206083\n",
      "train loss:0.273403260746\n",
      "train loss:0.232999685159\n",
      "train loss:0.397609795346\n",
      "train loss:0.263470251883\n",
      "train loss:0.273148637162\n",
      "train loss:0.3922993513\n",
      "train loss:0.521109077363\n",
      "train loss:0.378963195178\n",
      "train loss:0.240459004603\n",
      "train loss:0.231397971798\n",
      "train loss:0.378539523897\n",
      "train loss:0.179401395686\n",
      "train loss:0.264127912629\n",
      "train loss:0.221164214144\n",
      "train loss:0.178544494388\n",
      "train loss:0.213840850665\n",
      "train loss:0.286167991901\n",
      "train loss:0.356262705253\n",
      "train loss:0.344314836626\n",
      "train loss:0.335044312355\n",
      "train loss:0.305353019678\n",
      "train loss:0.172291369361\n",
      "train loss:0.278240107695\n",
      "train loss:0.287329318901\n",
      "=== epoch:4, train acc:0.886, test acc:0.889 ===\n",
      "train loss:0.186586495094\n",
      "train loss:0.279685991635\n",
      "train loss:0.279395774286\n",
      "train loss:0.212309484\n",
      "train loss:0.308397461376\n",
      "train loss:0.239677872174\n",
      "train loss:0.320097623638\n",
      "train loss:0.437576377062\n",
      "train loss:0.159462877189\n",
      "train loss:0.214455204118\n",
      "train loss:0.152737300503\n",
      "train loss:0.344387414027\n",
      "train loss:0.203625851453\n",
      "train loss:0.195657744794\n",
      "train loss:0.223701525509\n",
      "train loss:0.216322334293\n",
      "train loss:0.24079549529\n",
      "train loss:0.252837319967\n",
      "train loss:0.218567733753\n",
      "train loss:0.526852511029\n",
      "train loss:0.205924795959\n",
      "train loss:0.283483449198\n",
      "train loss:0.159016922347\n",
      "train loss:0.150414551203\n",
      "train loss:0.322638715052\n",
      "train loss:0.29592576235\n",
      "train loss:0.21359793149\n",
      "train loss:0.172667983178\n",
      "train loss:0.309019020912\n",
      "train loss:0.211582617513\n",
      "train loss:0.167985669013\n",
      "train loss:0.229267839421\n",
      "train loss:0.218916739795\n",
      "train loss:0.170588364789\n",
      "train loss:0.132680993636\n",
      "train loss:0.313717540671\n",
      "train loss:0.157786094988\n",
      "train loss:0.193055173982\n",
      "train loss:0.199532870022\n",
      "train loss:0.370613666833\n",
      "train loss:0.228317467685\n",
      "train loss:0.187576101263\n",
      "train loss:0.29356670225\n",
      "train loss:0.171066400794\n",
      "train loss:0.178230220482\n",
      "train loss:0.207914537114\n",
      "train loss:0.197281516897\n",
      "train loss:0.363024831394\n",
      "train loss:0.236755687762\n",
      "train loss:0.203663401863\n",
      "=== epoch:5, train acc:0.909, test acc:0.898 ===\n",
      "train loss:0.208823150076\n",
      "train loss:0.227506124225\n",
      "train loss:0.168963268364\n",
      "train loss:0.3760215234\n",
      "train loss:0.340561846007\n",
      "train loss:0.227543905503\n",
      "train loss:0.216702120665\n",
      "train loss:0.155467658746\n",
      "train loss:0.235146174349\n",
      "train loss:0.235880497302\n",
      "train loss:0.218912446947\n",
      "train loss:0.15271906458\n",
      "train loss:0.404665325208\n",
      "train loss:0.336507725928\n",
      "train loss:0.235502522589\n",
      "train loss:0.374598321899\n",
      "train loss:0.189217928834\n",
      "train loss:0.223560520732\n",
      "train loss:0.150850208639\n",
      "train loss:0.187175550662\n",
      "train loss:0.215350788222\n",
      "train loss:0.180423272924\n",
      "train loss:0.16798833461\n",
      "train loss:0.279117792504\n",
      "train loss:0.227741983704\n",
      "train loss:0.1469736823\n",
      "train loss:0.225195755496\n",
      "train loss:0.146526763493\n",
      "train loss:0.232410062994\n",
      "train loss:0.198562965781\n",
      "train loss:0.201367800518\n",
      "train loss:0.160364642995\n",
      "train loss:0.155058166575\n",
      "train loss:0.23242870848\n",
      "train loss:0.314680885306\n",
      "train loss:0.142031335184\n",
      "train loss:0.251152519848\n",
      "train loss:0.200234021482\n",
      "train loss:0.201847108147\n",
      "train loss:0.296620934856\n",
      "train loss:0.155561178814\n",
      "train loss:0.17404958916\n",
      "train loss:0.247197230643\n",
      "train loss:0.190308281816\n",
      "train loss:0.182747695978\n",
      "train loss:0.206617268799\n",
      "train loss:0.148623604991\n",
      "train loss:0.214174442308\n",
      "train loss:0.178256784477\n",
      "train loss:0.189487751502\n",
      "=== epoch:6, train acc:0.923, test acc:0.923 ===\n",
      "train loss:0.209476387661\n",
      "train loss:0.0946301346528\n",
      "train loss:0.152886467303\n",
      "train loss:0.220140417625\n",
      "train loss:0.24991479625\n",
      "train loss:0.22617965678\n",
      "train loss:0.146090146534\n",
      "train loss:0.114869498871\n",
      "train loss:0.217527155963\n",
      "train loss:0.275222227947\n",
      "train loss:0.184073535911\n",
      "train loss:0.126418600944\n",
      "train loss:0.264090400866\n",
      "train loss:0.165321446273\n",
      "train loss:0.155290832984\n",
      "train loss:0.108258563211\n",
      "train loss:0.268048887417\n",
      "train loss:0.281442634333\n",
      "train loss:0.27082060054\n",
      "train loss:0.192726669011\n",
      "train loss:0.117657196585\n",
      "train loss:0.213032009058\n",
      "train loss:0.176028832298\n",
      "train loss:0.0929029810899\n",
      "train loss:0.192455868947\n",
      "train loss:0.349265648987\n",
      "train loss:0.0937964420067\n",
      "train loss:0.221292088243\n",
      "train loss:0.22649613896\n",
      "train loss:0.198294026119\n",
      "train loss:0.218795723668\n",
      "train loss:0.169499356832\n",
      "train loss:0.0648935208536\n",
      "train loss:0.181480403187\n",
      "train loss:0.0701376695814\n",
      "train loss:0.237367730024\n",
      "train loss:0.168883182379\n",
      "train loss:0.189190713314\n",
      "train loss:0.120145259869\n",
      "train loss:0.169341128393\n",
      "train loss:0.176243107125\n",
      "train loss:0.277497058201\n",
      "train loss:0.197939224524\n",
      "train loss:0.318372782045\n",
      "train loss:0.293805453008\n",
      "train loss:0.190691424423\n",
      "train loss:0.181693640932\n",
      "train loss:0.185409661229\n",
      "train loss:0.144360510802\n",
      "train loss:0.271303438397\n",
      "=== epoch:7, train acc:0.941, test acc:0.921 ===\n",
      "train loss:0.209069770132\n",
      "train loss:0.203039274671\n",
      "train loss:0.177323462119\n",
      "train loss:0.0933450349219\n",
      "train loss:0.259709384185\n",
      "train loss:0.169642838056\n",
      "train loss:0.189987889543\n",
      "train loss:0.131386098344\n",
      "train loss:0.137564226778\n",
      "train loss:0.173965338058\n",
      "train loss:0.10052041989\n",
      "train loss:0.194248087302\n",
      "train loss:0.127471339687\n",
      "train loss:0.146037386602\n",
      "train loss:0.163691977068\n",
      "train loss:0.144403971792\n",
      "train loss:0.326335300257\n",
      "train loss:0.254590378041\n",
      "train loss:0.22004167291\n",
      "train loss:0.0873888301837\n",
      "train loss:0.200043471507\n",
      "train loss:0.1066043082\n",
      "train loss:0.0848240878564\n",
      "train loss:0.21978642468\n",
      "train loss:0.138205313673\n",
      "train loss:0.165732454152\n",
      "train loss:0.184362072269\n",
      "train loss:0.117963963149\n",
      "train loss:0.114429354209\n",
      "train loss:0.136915289752\n",
      "train loss:0.221580818761\n",
      "train loss:0.108527214144\n",
      "train loss:0.320287075241\n",
      "train loss:0.117414375141\n",
      "train loss:0.203356624765\n",
      "train loss:0.135669745714\n",
      "train loss:0.116586554789\n",
      "train loss:0.151962134487\n",
      "train loss:0.195477706354\n",
      "train loss:0.126789300179\n",
      "train loss:0.172564259186\n",
      "train loss:0.304023532026\n",
      "train loss:0.188163623043\n",
      "train loss:0.122779816521\n",
      "train loss:0.229104155312\n",
      "train loss:0.202444935055\n",
      "train loss:0.17318696792\n",
      "train loss:0.087682180209\n",
      "train loss:0.106163287308\n",
      "train loss:0.0983513019376\n",
      "=== epoch:8, train acc:0.946, test acc:0.931 ===\n",
      "train loss:0.187639547654\n",
      "train loss:0.113556388051\n",
      "train loss:0.0408309516646\n",
      "train loss:0.0521766427761\n",
      "train loss:0.148154959694\n",
      "train loss:0.155911053933\n",
      "train loss:0.192668197125\n",
      "train loss:0.110201733616\n",
      "train loss:0.148247048102\n",
      "train loss:0.124663877304\n",
      "train loss:0.116835543095\n",
      "train loss:0.160183510225\n",
      "train loss:0.0854021472829\n",
      "train loss:0.231133250445\n",
      "train loss:0.113156676829\n",
      "train loss:0.208960888134\n",
      "train loss:0.109521842211\n",
      "train loss:0.125908588742\n",
      "train loss:0.285299044199\n",
      "train loss:0.176617085581\n",
      "train loss:0.153637415163\n",
      "train loss:0.0753773492346\n",
      "train loss:0.187996568256\n",
      "train loss:0.113663196945\n",
      "train loss:0.106487095317\n",
      "train loss:0.124734388491\n",
      "train loss:0.14658264751\n",
      "train loss:0.157083845704\n",
      "train loss:0.25554120084\n",
      "train loss:0.0731037422442\n",
      "train loss:0.141475119719\n",
      "train loss:0.130827793947\n",
      "train loss:0.096107796161\n",
      "train loss:0.115939231175\n",
      "train loss:0.103397895423\n",
      "train loss:0.12830622474\n",
      "train loss:0.139626242126\n",
      "train loss:0.13002933158\n",
      "train loss:0.0497920820425\n",
      "train loss:0.0754839945306\n",
      "train loss:0.126825158305\n",
      "train loss:0.0965890414639\n",
      "train loss:0.146111421751\n",
      "train loss:0.0887828447303\n",
      "train loss:0.0777123701125\n",
      "train loss:0.0565889306234\n",
      "train loss:0.0833453055523\n",
      "train loss:0.170890153889\n",
      "train loss:0.130571986621\n",
      "train loss:0.112341358349\n",
      "=== epoch:9, train acc:0.947, test acc:0.934 ===\n",
      "train loss:0.156330474824\n",
      "train loss:0.0623942786815\n",
      "train loss:0.0702334130754\n",
      "train loss:0.0475828155182\n",
      "train loss:0.117742382962\n",
      "train loss:0.243828889421\n",
      "train loss:0.16441784901\n",
      "train loss:0.078002083633\n",
      "train loss:0.207877230849\n",
      "train loss:0.0600026859289\n",
      "train loss:0.157563588556\n",
      "train loss:0.165423447236\n",
      "train loss:0.113617178359\n",
      "train loss:0.165726013109\n",
      "train loss:0.111810961527\n",
      "train loss:0.117824071021\n",
      "train loss:0.173004302533\n",
      "train loss:0.240068672688\n",
      "train loss:0.145547248552\n",
      "train loss:0.181402200886\n",
      "train loss:0.18943577068\n",
      "train loss:0.202550684664\n",
      "train loss:0.269471228699\n",
      "train loss:0.126514014493\n",
      "train loss:0.100974785455\n",
      "train loss:0.0963634177311\n",
      "train loss:0.129889343024\n",
      "train loss:0.08327406342\n",
      "train loss:0.145757364692\n",
      "train loss:0.20190490944\n",
      "train loss:0.101267079637\n",
      "train loss:0.159477606345\n",
      "train loss:0.14418034618\n",
      "train loss:0.0987629966383\n",
      "train loss:0.139514759652\n",
      "train loss:0.155725527321\n",
      "train loss:0.128128215989\n",
      "train loss:0.107206577861\n",
      "train loss:0.253824874819\n",
      "train loss:0.111005281206\n",
      "train loss:0.151910700522\n",
      "train loss:0.165182610156\n",
      "train loss:0.163933486508\n",
      "train loss:0.0798046628458\n",
      "train loss:0.127947391471\n",
      "train loss:0.0912821959527\n",
      "train loss:0.106856985172\n",
      "train loss:0.15691337976\n",
      "train loss:0.111046582382\n",
      "train loss:0.0543655986255\n",
      "=== epoch:10, train acc:0.959, test acc:0.936 ===\n",
      "train loss:0.0991041318826\n",
      "train loss:0.136506056874\n",
      "train loss:0.0957029350314\n",
      "train loss:0.0734271324706\n",
      "train loss:0.0812935679462\n",
      "train loss:0.086437932427\n",
      "train loss:0.144064301802\n",
      "train loss:0.166516211365\n",
      "train loss:0.113955952167\n",
      "train loss:0.0756648281986\n",
      "train loss:0.163231630672\n",
      "train loss:0.0589363630371\n",
      "train loss:0.0942296704813\n",
      "train loss:0.0648857738036\n",
      "train loss:0.0913495619925\n",
      "train loss:0.0939159156096\n",
      "train loss:0.121542800783\n",
      "train loss:0.0871592860895\n",
      "train loss:0.109636690367\n",
      "train loss:0.0607929543154\n",
      "train loss:0.0574277138217\n",
      "train loss:0.0665836859113\n",
      "train loss:0.0555005624534\n",
      "train loss:0.0861328977108\n",
      "train loss:0.190131188201\n",
      "train loss:0.162773236539\n",
      "train loss:0.139660067329\n",
      "train loss:0.182389754678\n",
      "train loss:0.0982903583025\n",
      "train loss:0.0825546516424\n",
      "train loss:0.186037718236\n",
      "train loss:0.0806132649569\n",
      "train loss:0.174245077048\n",
      "train loss:0.0943131701381\n",
      "train loss:0.106439498249\n",
      "train loss:0.128200152332\n",
      "train loss:0.186433140734\n",
      "train loss:0.0349233938731\n",
      "train loss:0.135368163292\n",
      "train loss:0.0977624522653\n",
      "train loss:0.061467300884\n",
      "train loss:0.107491943935\n",
      "train loss:0.10613474936\n",
      "train loss:0.139296340195\n",
      "train loss:0.0736161844279\n",
      "train loss:0.0890383373277\n",
      "train loss:0.139870608524\n",
      "train loss:0.044931307163\n",
      "train loss:0.108581358992\n",
      "train loss:0.0543409273217\n",
      "=== epoch:11, train acc:0.961, test acc:0.938 ===\n",
      "train loss:0.0877760399215\n",
      "train loss:0.0974880161117\n",
      "train loss:0.0538356049687\n",
      "train loss:0.141787305911\n",
      "train loss:0.055178509971\n",
      "train loss:0.145873249121\n",
      "train loss:0.110027870969\n",
      "train loss:0.117385234528\n",
      "train loss:0.130259553434\n",
      "train loss:0.0893875000693\n",
      "train loss:0.0452877258422\n",
      "train loss:0.0807089089021\n",
      "train loss:0.0493711096846\n",
      "train loss:0.109811581509\n",
      "train loss:0.19649328908\n",
      "train loss:0.0977358891628\n",
      "train loss:0.0562373377483\n",
      "train loss:0.0956531870892\n",
      "train loss:0.0530114896444\n",
      "train loss:0.138549712328\n",
      "train loss:0.0403835173623\n",
      "train loss:0.0626492320734\n",
      "train loss:0.0590045378837\n",
      "train loss:0.0853064073767\n",
      "train loss:0.0714302301591\n",
      "train loss:0.143352369472\n",
      "train loss:0.0846404361246\n",
      "train loss:0.0783112313944\n",
      "train loss:0.0976380572161\n",
      "train loss:0.0739452841317\n",
      "train loss:0.0750449499422\n",
      "train loss:0.0867833328541\n",
      "train loss:0.0920650726927\n",
      "train loss:0.145752295264\n",
      "train loss:0.142936013903\n",
      "train loss:0.058392293092\n",
      "train loss:0.0842440155399\n",
      "train loss:0.173754581589\n",
      "train loss:0.0711458006917\n",
      "train loss:0.0485465888197\n",
      "train loss:0.0869608935332\n",
      "train loss:0.0827475174129\n",
      "train loss:0.069639411033\n",
      "train loss:0.0962175836929\n",
      "train loss:0.0634709269342\n",
      "train loss:0.0439011336563\n",
      "train loss:0.0663232105301\n",
      "train loss:0.11613377095\n",
      "train loss:0.0678441072476\n",
      "train loss:0.122520292562\n",
      "=== epoch:12, train acc:0.968, test acc:0.945 ===\n",
      "train loss:0.14505285531\n",
      "train loss:0.0944086690074\n",
      "train loss:0.0940399342488\n",
      "train loss:0.0602594083954\n",
      "train loss:0.0771747927949\n",
      "train loss:0.0756033041292\n",
      "train loss:0.0559504176282\n",
      "train loss:0.0545033606342\n",
      "train loss:0.0879051883569\n",
      "train loss:0.0739869708165\n",
      "train loss:0.0545385419534\n",
      "train loss:0.0484134799456\n",
      "train loss:0.127175219818\n",
      "train loss:0.0646960457244\n",
      "train loss:0.0497871553345\n",
      "train loss:0.0565660988878\n",
      "train loss:0.170862735165\n",
      "train loss:0.0376493029345\n",
      "train loss:0.070777060954\n",
      "train loss:0.102930869586\n",
      "train loss:0.0936855167384\n",
      "train loss:0.133665200158\n",
      "train loss:0.0639275132183\n",
      "train loss:0.090754548101\n",
      "train loss:0.0993353292338\n",
      "train loss:0.0767025862737\n",
      "train loss:0.0419987487319\n",
      "train loss:0.0508352567133\n",
      "train loss:0.0456890606346\n",
      "train loss:0.0386365115885\n",
      "train loss:0.0665959255677\n",
      "train loss:0.099350158805\n",
      "train loss:0.0271980479094\n",
      "train loss:0.0308354674295\n",
      "train loss:0.0964156480881\n",
      "train loss:0.078997465912\n",
      "train loss:0.043213295135\n",
      "train loss:0.0686324631603\n",
      "train loss:0.0517013858637\n",
      "train loss:0.0814200707607\n",
      "train loss:0.0501045081628\n",
      "train loss:0.098182763753\n",
      "train loss:0.0590747495945\n",
      "train loss:0.18797393416\n",
      "train loss:0.0773189757699\n",
      "train loss:0.0291874416499\n",
      "train loss:0.0542517697658\n",
      "train loss:0.0429292926098\n",
      "train loss:0.105849357549\n",
      "train loss:0.174868429889\n",
      "=== epoch:13, train acc:0.962, test acc:0.941 ===\n",
      "train loss:0.0548486705254\n",
      "train loss:0.0738020754911\n",
      "train loss:0.0245930262856\n",
      "train loss:0.0633392397517\n",
      "train loss:0.0578259200107\n",
      "train loss:0.0597783418041\n",
      "train loss:0.0314872617344\n",
      "train loss:0.137844188017\n",
      "train loss:0.0440368610907\n",
      "train loss:0.0489142068061\n",
      "train loss:0.0607615495215\n",
      "train loss:0.0742541629351\n",
      "train loss:0.0358605378808\n",
      "train loss:0.0393009854817\n",
      "train loss:0.0576385545905\n",
      "train loss:0.0279137060668\n",
      "train loss:0.0297083645281\n",
      "train loss:0.0800264588803\n",
      "train loss:0.0363117211374\n",
      "train loss:0.0337733966171\n",
      "train loss:0.0549378547327\n",
      "train loss:0.0574568878152\n",
      "train loss:0.127124594781\n",
      "train loss:0.159720355503\n",
      "train loss:0.0676096768751\n",
      "train loss:0.0350982029232\n",
      "train loss:0.0722233840931\n",
      "train loss:0.10770082574\n",
      "train loss:0.0309970990967\n",
      "train loss:0.0303461196939\n",
      "train loss:0.140569962365\n",
      "train loss:0.102824621302\n",
      "train loss:0.0603829436645\n",
      "train loss:0.0530175323533\n",
      "train loss:0.0247610330194\n",
      "train loss:0.0452275029957\n",
      "train loss:0.0885582008608\n",
      "train loss:0.0394999516463\n",
      "train loss:0.108387835621\n",
      "train loss:0.0595073642083\n",
      "train loss:0.0770019111675\n",
      "train loss:0.04844964872\n",
      "train loss:0.0310143990958\n",
      "train loss:0.071416577251\n",
      "train loss:0.0521571459077\n",
      "train loss:0.0256225845348\n",
      "train loss:0.0432876584092\n",
      "train loss:0.0666158431869\n",
      "train loss:0.0647661723701\n",
      "train loss:0.0843993873161\n",
      "=== epoch:14, train acc:0.97, test acc:0.95 ===\n",
      "train loss:0.0314206697485\n",
      "train loss:0.109569389039\n",
      "train loss:0.0377681270453\n",
      "train loss:0.0924168264165\n",
      "train loss:0.06042437822\n",
      "train loss:0.0479695534371\n",
      "train loss:0.0316262861289\n",
      "train loss:0.0541080113835\n",
      "train loss:0.0486941119342\n",
      "train loss:0.0671508634831\n",
      "train loss:0.0236480230409\n",
      "train loss:0.0425668561407\n",
      "train loss:0.0489516019752\n",
      "train loss:0.0864864293416\n",
      "train loss:0.0221620569182\n",
      "train loss:0.0360887349178\n",
      "train loss:0.0277535118709\n",
      "train loss:0.0397886460524\n",
      "train loss:0.0860490486403\n",
      "train loss:0.0439169165763\n",
      "train loss:0.0569262326696\n",
      "train loss:0.030514055905\n",
      "train loss:0.0281450880435\n",
      "train loss:0.0422107535951\n",
      "train loss:0.0335731831925\n",
      "train loss:0.0971608833435\n",
      "train loss:0.0361746262481\n",
      "train loss:0.0487559150655\n",
      "train loss:0.050987746481\n",
      "train loss:0.100160381627\n",
      "train loss:0.0358848800916\n",
      "train loss:0.0713690030011\n",
      "train loss:0.0650326170539\n",
      "train loss:0.0441291375382\n",
      "train loss:0.131985962858\n",
      "train loss:0.129288650224\n",
      "train loss:0.140769633022\n",
      "train loss:0.0875720410115\n",
      "train loss:0.0758908705243\n",
      "train loss:0.0222278840021\n",
      "train loss:0.10809541017\n",
      "train loss:0.0675578821914\n",
      "train loss:0.120066402085\n",
      "train loss:0.0166023413077\n",
      "train loss:0.0500755090028\n",
      "train loss:0.050300032471\n",
      "train loss:0.0500138187005\n",
      "train loss:0.0540343155105\n",
      "train loss:0.0211213313359\n",
      "train loss:0.0805043110842\n",
      "=== epoch:15, train acc:0.983, test acc:0.947 ===\n",
      "train loss:0.0536019927304\n",
      "train loss:0.0322269015981\n",
      "train loss:0.0410556886289\n",
      "train loss:0.077702718861\n",
      "train loss:0.0556189970219\n",
      "train loss:0.160610113088\n",
      "train loss:0.0354880885274\n",
      "train loss:0.0484151394635\n",
      "train loss:0.044463021881\n",
      "train loss:0.0346848467347\n",
      "train loss:0.0155938504021\n",
      "train loss:0.0179454426748\n",
      "train loss:0.0790554163571\n",
      "train loss:0.0731185955077\n",
      "train loss:0.173020320744\n",
      "train loss:0.0277569034362\n",
      "train loss:0.0365146137073\n",
      "train loss:0.0484622685197\n",
      "train loss:0.0466982767981\n",
      "train loss:0.0459384195734\n",
      "train loss:0.0199138606156\n",
      "train loss:0.0399644759536\n",
      "train loss:0.0803142928384\n",
      "train loss:0.0506432124808\n",
      "train loss:0.0794287284586\n",
      "train loss:0.0615971533348\n",
      "train loss:0.0470340682575\n",
      "train loss:0.0286130582785\n",
      "train loss:0.0551497548653\n",
      "train loss:0.0408817307106\n",
      "train loss:0.0680358546588\n",
      "train loss:0.0877461458471\n",
      "train loss:0.063388833357\n",
      "train loss:0.0417762158531\n",
      "train loss:0.0508786216835\n",
      "train loss:0.0743263954526\n",
      "train loss:0.0305122251583\n",
      "train loss:0.037190843331\n",
      "train loss:0.0485748113199\n",
      "train loss:0.0195589633799\n",
      "train loss:0.0316825234698\n",
      "train loss:0.0586749666733\n",
      "train loss:0.0956730619941\n",
      "train loss:0.0466519612582\n",
      "train loss:0.0333995391077\n",
      "train loss:0.0267958574329\n",
      "train loss:0.0204805818184\n",
      "train loss:0.0394427754297\n",
      "train loss:0.0238993731481\n",
      "train loss:0.0183203293106\n",
      "=== epoch:16, train acc:0.974, test acc:0.95 ===\n",
      "train loss:0.076419131202\n",
      "train loss:0.0630657091611\n",
      "train loss:0.0583363518756\n",
      "train loss:0.0866512369852\n",
      "train loss:0.0140103024153\n",
      "train loss:0.042433637847\n",
      "train loss:0.061192199066\n",
      "train loss:0.015711319034\n",
      "train loss:0.13820518261\n",
      "train loss:0.0418683054595\n",
      "train loss:0.0363046487028\n",
      "train loss:0.0389390080226\n",
      "train loss:0.0595045079117\n",
      "train loss:0.052288908759\n",
      "train loss:0.0193715222669\n",
      "train loss:0.0485597055315\n",
      "train loss:0.0615134318498\n",
      "train loss:0.0388790428505\n",
      "train loss:0.0332688238714\n",
      "train loss:0.053667207672\n",
      "train loss:0.0396038487129\n",
      "train loss:0.0274319931679\n",
      "train loss:0.0312447152699\n",
      "train loss:0.0296437346664\n",
      "train loss:0.0656039805838\n",
      "train loss:0.0484782150802\n",
      "train loss:0.0746885560899\n",
      "train loss:0.0216197141691\n",
      "train loss:0.0322654262686\n",
      "train loss:0.150221235202\n",
      "train loss:0.0961375045181\n",
      "train loss:0.032733231466\n",
      "train loss:0.0614737176208\n",
      "train loss:0.0315596139648\n",
      "train loss:0.0206743721032\n",
      "train loss:0.037033283594\n",
      "train loss:0.0594594371426\n",
      "train loss:0.0385935327245\n",
      "train loss:0.0602366785528\n",
      "train loss:0.0916507139951\n",
      "train loss:0.0447091460023\n",
      "train loss:0.0530913377608\n",
      "train loss:0.0457363907294\n",
      "train loss:0.0239599070435\n",
      "train loss:0.0333292008359\n",
      "train loss:0.0586369609916\n",
      "train loss:0.0771207888619\n",
      "train loss:0.0218357558273\n",
      "train loss:0.0954827670095\n",
      "train loss:0.0330143711503\n",
      "=== epoch:17, train acc:0.971, test acc:0.939 ===\n",
      "train loss:0.0669902930456\n",
      "train loss:0.0754094825832\n",
      "train loss:0.027736088761\n",
      "train loss:0.0476385517525\n",
      "train loss:0.0391589451233\n",
      "train loss:0.113956078857\n",
      "train loss:0.105323145272\n",
      "train loss:0.0450431715275\n",
      "train loss:0.0185037104664\n",
      "train loss:0.06880375747\n",
      "train loss:0.0287585198611\n",
      "train loss:0.0391076974374\n",
      "train loss:0.0354263013977\n",
      "train loss:0.0901052955252\n",
      "train loss:0.0232223522719\n",
      "train loss:0.113648378541\n",
      "train loss:0.0151521762625\n",
      "train loss:0.022868086931\n",
      "train loss:0.0384888915568\n",
      "train loss:0.0840550513001\n",
      "train loss:0.0418686577627\n",
      "train loss:0.112023541449\n",
      "train loss:0.0420027783919\n",
      "train loss:0.0221792523438\n",
      "train loss:0.0284506166734\n",
      "train loss:0.0384626320359\n",
      "train loss:0.0506549375761\n",
      "train loss:0.0122170743426\n",
      "train loss:0.0566074792284\n",
      "train loss:0.0200867299029\n",
      "train loss:0.0324242738019\n",
      "train loss:0.0409134084301\n",
      "train loss:0.0248461927034\n",
      "train loss:0.0788272994112\n",
      "train loss:0.0219089746832\n",
      "train loss:0.0531960027669\n",
      "train loss:0.0490701312959\n",
      "train loss:0.0382596959504\n",
      "train loss:0.0259220206778\n",
      "train loss:0.00688873528915\n",
      "train loss:0.0300143249533\n",
      "train loss:0.032180706659\n",
      "train loss:0.0173996651688\n",
      "train loss:0.0913266981334\n",
      "train loss:0.0295607108744\n",
      "train loss:0.0557834247497\n",
      "train loss:0.0608129525306\n",
      "train loss:0.0403122050629\n",
      "train loss:0.0126872700811\n",
      "train loss:0.00994996947453\n",
      "=== epoch:18, train acc:0.984, test acc:0.952 ===\n",
      "train loss:0.0254812708557\n",
      "train loss:0.0210262276331\n",
      "train loss:0.0151756727201\n",
      "train loss:0.0523663400832\n",
      "train loss:0.0289759188334\n",
      "train loss:0.0333150840365\n",
      "train loss:0.098548245429\n",
      "train loss:0.0411653809531\n",
      "train loss:0.0216044107974\n",
      "train loss:0.0564236678489\n",
      "train loss:0.0272637764665\n",
      "train loss:0.0197471928212\n",
      "train loss:0.0519741241895\n",
      "train loss:0.0308563855199\n",
      "train loss:0.0252790579739\n",
      "train loss:0.0354999385202\n",
      "train loss:0.0656377092523\n",
      "train loss:0.0365833037753\n",
      "train loss:0.0206920551541\n",
      "train loss:0.0147839372547\n",
      "train loss:0.0199564461376\n",
      "train loss:0.0693961483717\n",
      "train loss:0.0415857143695\n",
      "train loss:0.05802984567\n",
      "train loss:0.0157075633012\n",
      "train loss:0.0240696824149\n",
      "train loss:0.0251927160543\n",
      "train loss:0.0338901002383\n",
      "train loss:0.0215725595348\n",
      "train loss:0.0186474026061\n",
      "train loss:0.00768348956766\n",
      "train loss:0.0155518551514\n",
      "train loss:0.116305992819\n",
      "train loss:0.0224118465705\n",
      "train loss:0.0580242401163\n",
      "train loss:0.0240043713635\n",
      "train loss:0.0216237542668\n",
      "train loss:0.0214294342617\n",
      "train loss:0.0275630400424\n",
      "train loss:0.0652259065538\n",
      "train loss:0.017593924815\n",
      "train loss:0.0255573710547\n",
      "train loss:0.0365757551934\n",
      "train loss:0.0337476244567\n",
      "train loss:0.0242669696419\n",
      "train loss:0.0074678235451\n",
      "train loss:0.0113889669583\n",
      "train loss:0.0168550579337\n",
      "train loss:0.0269312666621\n",
      "train loss:0.0813413187727\n",
      "=== epoch:19, train acc:0.981, test acc:0.947 ===\n",
      "train loss:0.0109152246462\n",
      "train loss:0.0381606592354\n",
      "train loss:0.0357902948278\n",
      "train loss:0.0249794835836\n",
      "train loss:0.0824237231464\n",
      "train loss:0.011072286541\n",
      "train loss:0.0796241168078\n",
      "train loss:0.0265451526003\n",
      "train loss:0.025737808575\n",
      "train loss:0.0130339641203\n",
      "train loss:0.0220671590863\n",
      "train loss:0.0170592274312\n",
      "train loss:0.0553868345798\n",
      "train loss:0.0296608643785\n",
      "train loss:0.0244856186293\n",
      "train loss:0.0166050252505\n",
      "train loss:0.0110428634652\n",
      "train loss:0.0547565931633\n",
      "train loss:0.0164428247023\n",
      "train loss:0.0172350967731\n",
      "train loss:0.0204600022888\n",
      "train loss:0.0228044838187\n",
      "train loss:0.0144179735177\n",
      "train loss:0.0591629587835\n",
      "train loss:0.0431238688665\n",
      "train loss:0.0453998506107\n",
      "train loss:0.0565545897615\n",
      "train loss:0.00978503159801\n",
      "train loss:0.0221405778774\n",
      "train loss:0.0182243816193\n",
      "train loss:0.071126084587\n",
      "train loss:0.0328592967718\n",
      "train loss:0.0551732871149\n",
      "train loss:0.0207526044018\n",
      "train loss:0.0503596346283\n",
      "train loss:0.00994099632753\n",
      "train loss:0.0770970399578\n",
      "train loss:0.0187956781217\n",
      "train loss:0.0148312196532\n",
      "train loss:0.0124256080851\n",
      "train loss:0.0143645193765\n",
      "train loss:0.0657789932421\n",
      "train loss:0.0354263236909\n",
      "train loss:0.0147962631885\n",
      "train loss:0.020688674343\n",
      "train loss:0.0259862044674\n",
      "train loss:0.0109690359102\n",
      "train loss:0.0367988879442\n",
      "train loss:0.0235873346502\n",
      "train loss:0.0178924822553\n",
      "=== epoch:20, train acc:0.99, test acc:0.952 ===\n",
      "train loss:0.0182156291108\n",
      "train loss:0.0368008848627\n",
      "train loss:0.0228774564137\n",
      "train loss:0.0215247874773\n",
      "train loss:0.0337718798292\n",
      "train loss:0.0209137206294\n",
      "train loss:0.019907071041\n",
      "train loss:0.0228239067004\n",
      "train loss:0.0233798746089\n",
      "train loss:0.0227307426642\n",
      "train loss:0.0779751199415\n",
      "train loss:0.0148108447943\n",
      "train loss:0.0341872731386\n",
      "train loss:0.0332998092576\n",
      "train loss:0.0196421748231\n",
      "train loss:0.0378930263937\n",
      "train loss:0.0127735591316\n",
      "train loss:0.0339014229468\n",
      "train loss:0.0136492380102\n",
      "train loss:0.0245646796418\n",
      "train loss:0.0268733189105\n",
      "train loss:0.0283765889361\n",
      "train loss:0.0110123424314\n",
      "train loss:0.0153413257057\n",
      "train loss:0.0391406159703\n",
      "train loss:0.0193249122392\n",
      "train loss:0.018073152876\n",
      "train loss:0.0113679584407\n",
      "train loss:0.0169419769793\n",
      "train loss:0.0488983015446\n",
      "train loss:0.0190889180073\n",
      "train loss:0.00799960593707\n",
      "train loss:0.0269115265932\n",
      "train loss:0.036192392165\n",
      "train loss:0.015059245764\n",
      "train loss:0.0464083614654\n",
      "train loss:0.0194797547745\n",
      "train loss:0.0366871130643\n",
      "train loss:0.0202647437401\n",
      "train loss:0.0316362669796\n",
      "train loss:0.0144841587594\n",
      "train loss:0.0485869887958\n",
      "train loss:0.0374653501696\n",
      "train loss:0.0141804149904\n",
      "train loss:0.0212774183437\n",
      "train loss:0.0196599496242\n",
      "train loss:0.00243929760896\n",
      "train loss:0.0088298486787\n",
      "train loss:0.0268372359505\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.955\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVtVJDskuQo/cAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118216668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 計算時間を減らすため、データを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
